{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def myscraper(url):\n",
    "    datalink1=\"https://dev.socrata.com/foundry/data.cityofnewyork.us/\"\n",
    "    datalink2='https://data.cityofnewyork.us/resource/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links=soup.findAll(\"div\", {\"class\": \"browse2-result\"})\n",
    "    dict2={}\n",
    "    count=0\n",
    "    for link in links:\n",
    "        try:\n",
    "            href=link.findAll(\"a\",{\"class\":\"browse2-result-api-link\"})[0][\"href\"].replace(datalink1,datalink2)+\".json\"\n",
    "            title=link.findAll(\"a\",{\"class\":\"browse2-result-name-link\"})[0].string\n",
    "            dict2[title]=href\n",
    "            count=count+1\n",
    "        except IndexError:\n",
    "            pass\n",
    "            #print(link.findAll(\"a\",{\"class\":\"browse2-result-name-link\"})[0].string)\n",
    "    print(count)\n",
    "    for key in dict2:\n",
    "        try:\n",
    "            data=pd.read_json(dict2[key])\n",
    "            print(key)\n",
    "            data.to_csv(r\"nyc/\"+re.sub(\"/\",\"\",key)+\".csv\",sep=\",\")\n",
    "        except:\n",
    "            print(\"error in file location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks(keyword):\n",
    "    link_list=[]\n",
    "    count=1\n",
    "    first_url=\"https://data.cityofnewyork.us/browse?amp=&q=\"+keyword+\"&sortBy=relevance&page=1\"\n",
    "    link_list.append(first_url)\n",
    "    paginationlink = requests.get(first_url)\n",
    "    soup = BeautifulSoup(paginationlink.text, \"html.parser\")\n",
    "    links=soup.findAll(\"a\",{\"class\":\"pageLink\"})\n",
    "    for link in links:\n",
    "        link_list.append(\"https://data.cityofnewyork.us\"+link['href'])\n",
    "        count=count+1\n",
    "    return link_list\n",
    "list=getlinks(\"education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2014-2015 School Closure Discharge Reporting - GPA\n",
      "2011-2012 School Closure Discharge Reporting - GPA\n",
      "2010-2011 School Closure Discharge Reporting GPA\n"
     ]
    }
   ],
   "source": [
    "for url in getlinks(\"gpa\"):\n",
    "    myscraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
